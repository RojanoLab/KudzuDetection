{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mn3-E6KA-gTD"
   },
   "source": [
    "# Image Classification Using a Deep CNN\n",
    "\n",
    "In this notebook, we classify images between alive kudzu images and dead kudzu images.\n",
    "\n",
    "The code in this notebook is based on the repository: [ImageClassification](https://github.com/nicknochnack/ImageClassification/tree/main).\n",
    "\n",
    "You can also refer to the tutorial on YouTube: [Image Classification Tutorial](https://youtu.be/jztwpsIzEGc).\n",
    "\n",
    "Before running the code, make sure to download the following file and save it to your AlivevsDeadKudzu repository:\n",
    "- [Alive vs Dead Kudzu Dataset](https://www.kaggle.com/datasets/albaclosatarres/alive-vs-dead-kudzu-vegetation/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKDRwnPJBs7V"
   },
   "source": [
    "## 1. Setup Requirements and Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JPTzhfUBB0v8"
   },
   "source": [
    "### 1.1 Install Dependencies and Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow this tutorial if want to use GPU https://www.xda-developers.com/use-gpu-jupyter-notebook/  \n",
    "Obtain your pytorch version here: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows \n",
    "# !pip install opencv-python matplotlib \n",
    "\n",
    "# MacOS\n",
    "# Run this command on the terminal: !conda install opencv matplotlib "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2812,
     "status": "ok",
     "timestamp": 1713378385390,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "XXAglMfnCUmk",
    "outputId": "5875f9b6-b837-4a33-e566-e3c70c46a4b1"
   },
   "outputs": [],
   "source": [
    "# check that librareis where installed correctly. You need: pytorch, , opencv-python and matplotlib\n",
    "# Windows \n",
    "!pip list\n",
    "\n",
    "# MacOS\n",
    "#!conda list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPU Correct Set Up Confirmation (skip if wanted to use CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"Found {torch.cuda.device_count()} GPU(s):\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"Failed to detect a GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (GPU or CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSoF12g7B7vE"
   },
   "source": [
    "### 1.2 Remove Dodgy Images\n",
    "Mislabeld images, currupt images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 233,
     "status": "ok",
     "timestamp": 1713378385803,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "vA868iFaIV0a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import imghdr\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1713378385804,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "gzyAXIOjBFO0"
   },
   "outputs": [],
   "source": [
    "data_dir = 'data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your_directory = \"YOUR_DIRECTORY_GOES_HERE\"\n",
    "your_directory = \"/home/student/Desktop/KudzuClassification/ObjectDetection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining paths using os.path.join()\n",
    "data_full_path = os.path.join(your_directory, data_dir)\n",
    "\n",
    "# Display the joined path\n",
    "print(\"Joined path:\", data_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713378385804,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "L_rWwg_NIpMJ"
   },
   "outputs": [],
   "source": [
    "image_exts = ['jpeg','jpg', 'bmp', 'png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10817,
     "status": "ok",
     "timestamp": 1713378396618,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "wnieCrK6IsWx"
   },
   "outputs": [],
   "source": [
    "# Windows\n",
    "for image_class in os.listdir(data_full_path):\n",
    "    for image in os.listdir(os.path.join(data_full_path, image_class)):\n",
    "        image_path = os.path.join(data_full_path, image_class, image)\n",
    "        try:\n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in image_exts:\n",
    "                print('Image not in ext list {}'.format(image_path))\n",
    "                os.remove(image_path)\n",
    "        except Exception as e:\n",
    "            print('Issue with image {}'.format(image_path))\n",
    "            os.remove(image_path)\n",
    "\"\"\"           \n",
    "# MacOS\n",
    "for image_class in os.listdir(data_full_path):\n",
    "    class_path = os.path.join(data_full_path, image_class)\n",
    "    # Check if the current path is a directory\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "    for image in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_path, image)\n",
    "        try:\n",
    "            # Process the image\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {image_path}: {str(e)}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pvIjRpOecuY2"
   },
   "source": [
    "### 1.3 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1713378396618,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "Db4BYxhuiWEE"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_images_to_1x1(folder_path, output_folder=None):\n",
    "    \"\"\"\n",
    "    Crop images in a folder to 1x1 aspect ratio if they are not close to 1x1.\n",
    "    \n",
    "    Args:\n",
    "    - folder_path (str): Path to the folder containing images.\n",
    "    - output_folder (str): Optional. Path to the folder to save cropped images. If not provided, images are displayed.\n",
    "    \"\"\"\n",
    "    # Create output folder if it doesn't exist\n",
    "    if output_folder and not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # List all files in the folder\n",
    "    image_files = os.listdir(folder_path)\n",
    "    num_images = len(image_files)\n",
    "    print(f\"Found {num_images} images in the folder {folder_path}\")\n",
    "    \n",
    "    # Process each image\n",
    "    for i, filename in enumerate(image_files):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        img = cv2.imread(image_path)\n",
    "        \n",
    "        if img is None:\n",
    "            print(f\"Error reading image {filename}. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Get image dimensions\n",
    "        height, width, _ = img.shape\n",
    "        \n",
    "        # Calculate aspect ratio\n",
    "        aspect_ratio = width / height\n",
    "        \n",
    "        print(f\"{filename} height= {height}, width= {width}, aspect_ratio= {aspect_ratio}  \")\n",
    "        \n",
    "        # Define the target size\n",
    "        target_size = max(width, height)\n",
    "        \n",
    "        # Check aspect ratio and crop if necessary\n",
    "        if aspect_ratio != 1:\n",
    "            # Determine the region of interest (ROI) for cropping\n",
    "            if width > height:\n",
    "                # Landscape orientation\n",
    "                crop_width = height\n",
    "                crop_height = height\n",
    "                start_x = (width - crop_width) // 2\n",
    "                start_y = 0\n",
    "            else:\n",
    "                # Portrait orientation or square\n",
    "                crop_width = width\n",
    "                crop_height = width\n",
    "                start_x = 0\n",
    "                start_y = (height - crop_height) // 2\n",
    "            \n",
    "            # Crop the image\n",
    "            cropped_img = img[start_y:start_y+crop_height, start_x:start_x+crop_width]\n",
    "            \n",
    "            # Resize cropped image to 1x1 aspect ratio\n",
    "            resized_img = cv2.resize(cropped_img, (target_size, target_size), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "            height, width, _ = resized_img.shape\n",
    "            aspect_ratio = width / height\n",
    "        \n",
    "            print(f\"{filename} new height= {height}, width= {width}, aspect_ratio= {aspect_ratio}  \")\n",
    "        \n",
    "        else:\n",
    "            # If aspect ratio is close to 1x1, use the original image\n",
    "            resized_img = img\n",
    "        \n",
    "        # Save or display the resized image\n",
    "        if output_folder:\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            cv2.imwrite(output_path, resized_img)\n",
    "        print(f\"Finished resizing {filename}.\")\n",
    "    \n",
    "    print(f\"Finished resizing all images.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining paths using os.path.join()\n",
    "data_full_path_alive = os.path.join(data_full_path, 'aliveKudzu')\n",
    "data_full_path_dead = os.path.join(data_full_path, 'deadKudzu')\n",
    "\n",
    "# Display the joined path\n",
    "print(\"Joined path alive kudzu:\", data_full_path_alive)\n",
    "print(\"Joined path dead kudzu:\", data_full_path_dead)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_images_to_1x1(data_full_path_alive, data_full_path_alive)\n",
    "crop_images_to_1x1(data_full_path_dead, data_full_path_dead)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A-yDJ4oojQhX"
   },
   "source": [
    "# 2. Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define the transform with resizing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Load dataset from directory\n",
    "dataset = datasets.ImageFolder(root='data', transform=transform)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "data_loader = DataLoader(dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713378396881,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "Q5W34BZ9jc4a"
   },
   "outputs": [],
   "source": [
    "# Get a batch of data\n",
    "data_iterator = iter(data_loader)\n",
    "images, labels = next(data_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1713378400375,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "cGCW4bNXmDGL",
    "outputId": "2b4411b1-70cc-44c4-f62e-7a2d67f4a70d"
   },
   "outputs": [],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 262
    },
    "executionInfo": {
     "elapsed": 3446,
     "status": "ok",
     "timestamp": 1713378403818,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "IUuusWojkMiB",
    "outputId": "11361b3a-45a6-4ce3-9ca0-0dd9b791e7e8"
   },
   "outputs": [],
   "source": [
    "# Display images using Matplotlib\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "for idx, img in enumerate(images[:4]):\n",
    "    # Convert tensor image to numpy and transpose channels from [C, H, W] to [H, W, C] for plotting\n",
    "    img = img.permute(1, 2, 0).numpy()\n",
    "    ax[idx].imshow(img)\n",
    "    ax[idx].title.set_text(labels[idx].item())  # Access the label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZ2UzT3ptMY9"
   },
   "source": [
    "### 2.1 Scale Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zaypp8uUwEY2"
   },
   "source": [
    "#### 2.1.1 Check that our data is scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(images.min())  # Should be 0 (if scaled properly)\n",
    "print(images.max())  # Should be 1 (if scaled properly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92_kcfl_tUX6"
   },
   "source": [
    "### 2.2 Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1713378414426,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "prvg8wG4tSyC"
   },
   "outputs": [],
   "source": [
    "# 300 + 150 images = 450\n",
    "# 80% train 10% validation 10% testing\n",
    "# Define the sizes for train, val, and test sets\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = int(0.1 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1713378414427,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "ni826oQRvX3Z",
    "outputId": "b9445293-d01d-414f-ef5c-ddf673f33bb2"
   },
   "outputs": [],
   "source": [
    "val_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1713378414427,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "zt2BnF77vZOf",
    "outputId": "12644ecb-9aec-40e6-9f04-874845ee0eb9"
   },
   "outputs": [],
   "source": [
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1713378414427,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "KbRP-IqbvNjD",
    "outputId": "6169f7cf-1014-4db8-b24b-edc5c4501a91"
   },
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1713378414427,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "oMzS6Zj1xm2e",
    "outputId": "347b0477-e9ae-4cb2-f4e3-576c207e2e97"
   },
   "outputs": [],
   "source": [
    "train_size+val_size+test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 17,
     "status": "ok",
     "timestamp": 1713378414427,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "jnEuPZi2vPzW"
   },
   "outputs": [],
   "source": [
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)+len(val_loader)+len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCII8WJSxzdc"
   },
   "source": [
    "# 3. Deep Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dG1Y02fex2-0"
   },
   "source": [
    "### 3.1 Build Deep Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the AlexNet model\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),  # (3, 512, 512) -> (16, 512, 512)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (16, 512, 512) -> (16, 256, 256)\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # (16, 256, 256) -> (32, 256, 256)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (32, 256, 256) -> (32, 128, 128)\n",
    "            \n",
    "            nn.Conv2d(32, 16, kernel_size=3, stride=1, padding=1),  # (32, 128, 128) -> (16, 128, 128)\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),  # (16, 128, 128) -> (32, 128, 128)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),  # (32, 128, 128) -> (32, 64, 64)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(32 * 64 * 64, 256),  # (32, 64, 64) -> 256\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "alexnet = AlexNet()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss\n",
    "optimizer = optim.Adam(alexnet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, expand_ratio, se_ratio):\n",
    "        super(MBConvBlock, self).__init__()\n",
    "        expanded_channels = in_channels * expand_ratio\n",
    "        self.expand = nn.Conv2d(in_channels, expanded_channels, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(expanded_channels)\n",
    "        self.dwconv = nn.Conv2d(expanded_channels, expanded_channels, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, groups=expanded_channels, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(expanded_channels)\n",
    "        self.se = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(expanded_channels, int(in_channels * se_ratio), kernel_size=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(int(in_channels * se_ratio), expanded_channels, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.project = nn.Conv2d(expanded_channels, out_channels, kernel_size=1, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.stride = stride\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu6(self.bn1(self.expand(x)))\n",
    "        x = F.relu6(self.bn2(self.dwconv(x)))\n",
    "        x = self.se(x) * x\n",
    "        x = self.bn3(self.project(x))\n",
    "        if self.in_channels == self.out_channels and self.stride == 1:\n",
    "            x = x + residual\n",
    "        return x\n",
    "\n",
    "class EfficientNetB0(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(EfficientNetB0, self).__init__()\n",
    "        \n",
    "        # Stem layer\n",
    "        self.stem = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # MBConv blocks (based on EfficientNet-B0 architecture)\n",
    "        self.block1 = MBConvBlock(32, 16, kernel_size=3, stride=1, expand_ratio=1, se_ratio=0.25)\n",
    "        self.block2 = MBConvBlock(16, 24, kernel_size=3, stride=2, expand_ratio=6, se_ratio=0.25)\n",
    "        self.block3 = MBConvBlock(24, 40, kernel_size=5, stride=2, expand_ratio=6, se_ratio=0.25)\n",
    "        self.block4 = MBConvBlock(40, 80, kernel_size=3, stride=2, expand_ratio=6, se_ratio=0.25)\n",
    "        self.block5 = MBConvBlock(80, 112, kernel_size=5, stride=1, expand_ratio=6, se_ratio=0.25)\n",
    "        self.block6 = MBConvBlock(112, 192, kernel_size=5, stride=2, expand_ratio=6, se_ratio=0.25)\n",
    "        self.block7 = MBConvBlock(192, 320, kernel_size=3, stride=1, expand_ratio=6, se_ratio=0.25)\n",
    "        \n",
    "        # Global pooling and fully connected layer\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(320, num_classes)  # 320 channels after MBConv blocks\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.stem(x)\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.block7(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten the output\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)  # For binary classification\n",
    "\n",
    "# Instantiate the model\n",
    "efficientnet = EfficientNetB0()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross entropy for binary classification\n",
    "optimizer = optim.Adam(efficientnet.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Xception(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(Xception, self).__init__()\n",
    "        self.entry_flow = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.middle_flow = self._make_middle_flow(8, 64)  # Middle flow with 8 blocks\n",
    "        self.exit_flow = nn.Sequential(\n",
    "            nn.Conv2d(64, 1024, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1024, 1536, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(1536, 2048, kernel_size=3, padding=1, bias=False),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc = nn.Linear(2048, num_classes)\n",
    "    \n",
    "    def _make_middle_flow(self, num_blocks, in_channels):\n",
    "        layers = []\n",
    "        for _ in range(num_blocks):\n",
    "            layers.append(nn.Sequential(\n",
    "                nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, groups=in_channels, bias=False),\n",
    "                nn.BatchNorm2d(in_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv2d(in_channels, in_channels, kernel_size=1, bias=False),\n",
    "                nn.BatchNorm2d(in_channels)\n",
    "            ))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.entry_flow(x)\n",
    "        x = self.middle_flow(x)\n",
    "        x = self.exit_flow(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        return torch.sigmoid(x)\n",
    "\n",
    "# Instantiate the model\n",
    "xception = Xception()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(xception.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the basic residual block for ResNet-18\n",
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.downsample = downsample\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        \n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Define the full ResNet-18 model\n",
    "class ResNet18(nn.Module):\n",
    "    def __init__(self, num_classes=1):  # num_classes=1 for binary classification\n",
    "        super(ResNet18, self).__init__()\n",
    "        self.in_channels = 64\n",
    "        \n",
    "        # Initial convolutional layer\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        # Residual layers\n",
    "        self.layer1 = self._make_layer(64, 2, stride=1)  # 2 residual blocks\n",
    "        self.layer2 = self._make_layer(128, 2, stride=2)  # 2 residual blocks\n",
    "        self.layer3 = self._make_layer(256, 2, stride=2)  # 2 residual blocks\n",
    "        self.layer4 = self._make_layer(512, 2, stride=2)  # 2 residual blocks\n",
    "        \n",
    "        # Global average pooling and fully connected layer\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512, num_classes)  # Change from 1000 to 1 for binary classification\n",
    "    \n",
    "    def _make_layer(self, out_channels, blocks, stride=1):\n",
    "        downsample = None\n",
    "        if stride != 1 or self.in_channels != out_channels:\n",
    "            downsample = nn.Sequential(\n",
    "                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(BasicBlock(self.in_channels, out_channels, stride, downsample))\n",
    "        self.in_channels = out_channels\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(BasicBlock(self.in_channels, out_channels))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return torch.sigmoid(x)  # Use sigmoid for binary classification\n",
    "\n",
    "# Instantiate the model for binary classification\n",
    "resnet18 = ResNet18(num_classes=1).to(device)  # Assuming binary classification\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss for binary classification\n",
    "optimizer = torch.optim.Adam(resnet18.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eG66O3hyfp1"
   },
   "source": [
    "### 3.2 Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZfLjxe78GW8"
   },
   "source": [
    "Check we are running this code with GPU by running the following code. Can be run with CPU it will just take longer.\n",
    "- AlexNet: 157.22 seconds\n",
    "- EfficientNet: 170.99 seconds\n",
    "- Xception: 1411.99 seconds\n",
    "- ResNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import time  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SummaryWriter to log data for TensorBoard\n",
    "writer = SummaryWriter(log_dir='logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 323140,
     "status": "ok",
     "timestamp": 1713378737722,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "7uQ7rGoK2355",
    "outputId": "d189000e-69e6-44f1-9f58-c968bdaade28"
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs=20):\n",
    "    start_time = time.time()  # Start the timer\n",
    "\n",
    "    # Initialize lists to store metrics\n",
    "    train_loss_history = []\n",
    "    train_acc_history = []\n",
    "    val_loss_history = []\n",
    "    val_acc_history = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        # Training Loop\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.float().to(device)  # Convert labels to float\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs).squeeze()  # Squeeze the output to match the shape of labels\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = (outputs > 0.5).float()  # For binary classification\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        # Save training metrics for this epoch\n",
    "        train_loss_history.append(epoch_loss)\n",
    "        train_acc_history.append(epoch_acc.item())\n",
    "        \n",
    "        # Validation Loop\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.float().to(device)  # Convert labels to float\n",
    "\n",
    "                outputs = model(inputs).squeeze()  # Squeeze the output to match the shape of labels\n",
    "                loss = criterion(outputs, labels)\n",
    "                preds = (outputs > 0.5).float()\n",
    "\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "\n",
    "        # Save validation metrics for this epoch\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc.item())\n",
    "        \n",
    "        # Logging to TensorBoard\n",
    "        writer.add_scalar(f'{model.__class__.__name__}/Training Loss', epoch_loss, epoch)\n",
    "        writer.add_scalar(f'{model.__class__.__name__}/Training Accuracy', epoch_acc, epoch)\n",
    "        writer.add_scalar(f'{model.__class__.__name__}/Validation Loss', val_loss, epoch)\n",
    "        writer.add_scalar(f'{model.__class__.__name__}/Validation Accuracy', val_acc, epoch)\n",
    "\n",
    "        print(f'Epoch {epoch}/{epochs - 1}')\n",
    "        print(f'Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "        print(f'Val Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        print(f'--------------------------------')\n",
    "\n",
    "    end_time = time.time()  # End the timer\n",
    "    total_time = end_time - start_time\n",
    "    print(f'Training time for {model.__class__.__name__}: {total_time:.2f} seconds')\n",
    "    \n",
    "    # Return model and training history\n",
    "    return model, total_time, train_loss_history, train_acc_history, val_loss_history, val_acc_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer for each model\n",
    "criterion = nn.BCELoss()  # Binary cross-entropy loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=0.0002 # entre 0.001 i 0.0001 provar 0.0005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "alexnet = AlexNet().to(device)\n",
    "optimizer_AN = torch.optim.Adam(alexnet.parameters(), lr=lr)\n",
    "trained_alexnet, alexnet_time, alexnet_train_loss, alexnet_train_acc, alexnet_val_loss, alexnet_val_acc = train_model(alexnet, \n",
    "                                                                                                                      train_loader, val_loader, criterion, optimizer_AN, epochs=20)\n",
    "times['AlexNet'] = alexnet_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "efficientnet = EfficientNetB0().to(device)\n",
    "optimizer_EN = torch.optim.Adam(efficientnet.parameters(), lr=lr)\n",
    "trained_efficientnet, efficientnet_time, efficientnet_train_loss, efficientnet_train_acc, efficientnet_val_loss, efficientnet_val_acc = train_model(efficientnet, train_loader, val_loader, criterion, optimizer_EN, epochs=20)\n",
    "times['EfficientNet'] = efficientnet_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xception = Xception().to(device)\n",
    "optimizer_X = torch.optim.Adam(xception.parameters(), lr=lr)\n",
    "trained_xception, xception_time, xception_train_loss, xception_train_acc, xception_val_loss, xception_val_acc = train_model(xception, train_loader, val_loader, criterion, optimizer_X, epochs=20)\n",
    "times['Xception'] = xception_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train resnet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18 = ResNet18().to(device)\n",
    "optimizer_RN = torch.optim.Adam(resnet18.parameters(), lr=lr)\n",
    "trained_resnet18, resnet18_time, resnet18_train_loss, resnet18_train_acc, resnet18_val_loss, resnet18_val_acc = train_model(resnet18, train_loader, val_loader, criterion, optimizer_RN, epochs=20)\n",
    "times['ResNet-50'] = resnet18_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print Out Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out training times\n",
    "for model_name, train_time in times.items():\n",
    "    print(f'{model_name} took {train_time:.2f} seconds to train')\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TP4naTnXyf2F"
   },
   "source": [
    "### 3.3 Plot Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_models(train, val, plot_type, model_names):\n",
    "    epochs = range(1, len(train[0]) + 1)\n",
    "\n",
    "    colors = {\n",
    "        'AlexNet': 'green',\n",
    "        'EfficientNet': 'orange',\n",
    "        'Xception': 'red',\n",
    "        'ResNet18': 'blue'\n",
    "    }\n",
    "    \n",
    "    # Create a figure for comparing loss values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot training and validation loss for each model\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        color = colors.get(model_name, 'black')  # Use black as default if the model name is not found\n",
    "        plt.plot(epochs, train[i], color=color,label=f'{model_name} Train {plot_type}')\n",
    "        plt.plot(epochs, val[i], '--', color=color,alpha=0.2, label=f'{model_name} Val {plot_type}')\n",
    "\n",
    "    plt.title(f'Training and Validation {plot_type} for Multiple Models')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(plot_type)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect training and validation losses into lists\n",
    "train_losses = [alexnet_train_loss, efficientnet_train_loss, resnet18_train_loss, xception_train_loss ]\n",
    "val_losses = [alexnet_val_loss, efficientnet_val_loss, resnet18_val_loss, xception_val_loss ]\n",
    "\n",
    "# Model names for plotting\n",
    "model_names = ['AlexNet', 'EfficientNet', 'ResNet18', 'Xception', ]\n",
    "\n",
    "# Plot loss comparison\n",
    "plot_multiple_models(train_losses, val_losses, 'Loss', model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect training and validation losses into lists\n",
    "train_acc = [alexnet_train_acc, efficientnet_train_acc, resnet18_train_acc , xception_train_acc ]\n",
    "val_acc = [alexnet_val_acc, efficientnet_val_acc, resnet18_val_acc , xception_val_acc ]\n",
    "\n",
    "# Model names for plotting\n",
    "model_names = ['AlexNet', 'EfficientNet', 'ResNet18', 'Xception' ]\n",
    "\n",
    "# Plot loss comparison\n",
    "plot_multiple_models(train_acc, val_acc, 'Accuracy', model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_models(train_loss, val_loss, train_acc, val_acc, model_names):\n",
    "    epochs = range(1, len(train_loss[0]) + 1)\n",
    "\n",
    "    colors = {\n",
    "        'AlexNet': 'green',\n",
    "        'EfficientNet': 'orange',\n",
    "        'Xception': 'red',\n",
    "        'ResNet18': 'blue'\n",
    "    }\n",
    "    \n",
    "    # Create a figure for comparing loss values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Plot training and validation loss for each model\n",
    "    for i, model_name in enumerate(model_names):\n",
    "        color = colors.get(model_name, 'black')  # Use black as default if the model name is not found\n",
    "        plt.plot(epochs, train_loss[i], color=color,label=f'{model_name} Train Loss')\n",
    "        plt.plot(epochs, val_loss[i], '--', color=color,alpha=0.4, label=f'{model_name} Val Loss')\n",
    "        plt.plot(epochs, train_acc[i], color=color,label=f'{model_name} Train Accuracy')\n",
    "        plt.plot(epochs, val_acc[i], '--', color=color,alpha=0.4, label=f'{model_name} Val Accuracy')\n",
    "\n",
    "    plt.title(f'Training and Validation Loss and Accuracy for Multiple Models')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss and Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect training and validation losses into lists\n",
    "train_loss = [alexnet_train_loss, efficientnet_train_loss, resnet18_train_loss, xception_train_loss ]\n",
    "val_loss = [alexnet_val_loss, efficientnet_val_loss, resnet18_val_loss, xception_val_loss ]\n",
    "\n",
    "\n",
    "# Collect training and validation losses into lists\n",
    "train_acc = [alexnet_train_acc, efficientnet_train_acc, resnet18_train_acc , xception_train_acc ]\n",
    "val_acc = [alexnet_val_acc, efficientnet_val_acc, resnet18_val_acc , xception_val_acc ]\n",
    "\n",
    "# Model names for plotting\n",
    "model_names = ['AlexNet', 'EfficientNet', 'ResNet18', 'Xception', ]\n",
    "\n",
    "# Plot loss comparison\n",
    "plot_multiple_models(train_loss, val_loss, train_acc, val_acc, model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize feature maps\n",
    "def visualize_feature_maps(model, img_tensor, layer_idx, num_filters=16):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Forward pass through the model up to the desired layer\n",
    "    activations = img_tensor\n",
    "    with torch.no_grad():\n",
    "        # Pass through layers up to the desired layer\n",
    "        for idx, layer in enumerate(model.children()):\n",
    "            activations = layer(activations)\n",
    "            if idx == layer_idx:  # Stop when we reach the desired layer\n",
    "                break\n",
    "\n",
    "    # Normalize the feature maps\n",
    "    activations = activations - activations.min()\n",
    "    activations = activations / activations.max()\n",
    "\n",
    "    # Plot the feature maps\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(12, 12))  # 4x4 grid for 16 filters\n",
    "    for i in range(num_filters):\n",
    "        row = i // 4\n",
    "        col = i % 4\n",
    "        ax = axes[row][col]\n",
    "        ax.imshow(activations[0, i].cpu().numpy(), cmap='viridis')  # Convert to numpy for visualization\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize feature maps for each convolutional layer\n",
    "def visualize_all_conv_layers(model, img_tensor, device):\n",
    "    img_tensor = img_tensor.to(device)  # Ensure image is on the correct device\n",
    "    conv_layers = get_conv_layers(model)\n",
    "    model_name = type(model).__name__\n",
    "\n",
    "    for i, layer in enumerate(conv_layers):\n",
    "        print(f\"Visualizing feature maps for layer {i + 1} in {model_name}\")\n",
    "        visualize_feature_maps(model, img_tensor, layer_idx=i)\n",
    "\n",
    "# Example usage:\n",
    "img_path = \"/home/student/Desktop/KudzuClassification/ObjectDetection/output/image_29.8300833_-81.31416568_90.jpg\"\n",
    "img_tensor = preprocess_image(img_path, target_size=(512, 512), device=device)  # Preprocess and move to the correct device\n",
    "models = [alexnet, efficientnet, resnet18, xception ]\n",
    "\n",
    "for model in models:\n",
    "    model = model.to(device)  # Move model to the correct device\n",
    "    visualize_all_conv_layers(model, img_tensor, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-Xx-VF2lUo8"
   },
   "source": [
    "# 4. Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsyXCQlnldLF"
   },
   "source": [
    "### 4.1 Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, roc_curve, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import torch\n",
    "\n",
    "# Step 1: Make Predictions\n",
    "def get_predictions(model, test_loader, device):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in test_loader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass: get predictions\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Assuming binary classification, convert probabilities to class labels\n",
    "            predicted_labels = (outputs > 0.5).cpu().numpy().astype(int)\n",
    "\n",
    "            # Collect true and predicted labels\n",
    "            y_true.extend(labels.cpu().numpy().flatten())  # Convert to numpy and flatten if necessary\n",
    "            y_pred.extend(predicted_labels.flatten())  # Flatten if necessary\n",
    "    \n",
    "    return np.array(y_true), np.array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate Metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    precision = precision_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    recall = recall_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='weighted', zero_division=1)\n",
    "    \n",
    "    return precision, recall, accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Perform 10 predictions and calculate the average metrics\n",
    "def average_predictions(model, test_loader, device, num_predictions=10):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    accuracies = []\n",
    "    f1_scores = []\n",
    "    \n",
    "    for _ in range(num_predictions):\n",
    "        y_true, y_pred = get_predictions(model, test_loader, device)\n",
    "        precision, recall, accuracy, f1 = calculate_metrics(y_true, y_pred)\n",
    "        \n",
    "        # Append metrics for each prediction run\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        accuracies.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "    \n",
    "    # Calculate average metrics\n",
    "    avg_precision = np.mean(precisions)\n",
    "    avg_recall = np.mean(recalls)\n",
    "    avg_accuracy = np.mean(accuracies)\n",
    "    avg_f1 = np.mean(f1_scores)\n",
    "    \n",
    "    return avg_precision, avg_recall, avg_accuracy, avg_f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage: Get average predictions for each model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Perform 10 predictions and get average metrics for each model\n",
    "avg_precision1, avg_recall1, avg_accuracy1, avg_f11 = average_predictions(alexnet, test_loader, device)\n",
    "avg_precision2, avg_recall2, avg_accuracy2, avg_f12 = average_predictions(efficientnet, test_loader, device)\n",
    "avg_precision3, avg_recall3, avg_accuracy3, avg_f13 = average_predictions(xception, test_loader, device)\n",
    "avg_precision4, avg_recall4, avg_accuracy4, avg_f14 = average_predictions(resnet18, test_loader, device)\n",
    "\n",
    "# Step 4: Print Average Results in percentages with two decimal places\n",
    "print(f\"AlexNet - Avg Precision: {avg_precision1 * 100:.2f}%, Avg Recall: {avg_recall1 * 100:.2f}%, Avg Accuracy: {avg_accuracy1 * 100:.2f}%, Avg F1 Score: {avg_f11 * 100:.2f}%\")\n",
    "print(f\"EfficientNet - Avg Precision: {avg_precision2 * 100:.2f}%, Avg Recall: {avg_recall2 * 100:.2f}%, Avg Accuracy: {avg_accuracy2 * 100:.2f}%, Avg F1 Score: {avg_f12 * 100:.2f}%\")\n",
    "print(f\"Xception - Avg Precision: {avg_precision3 * 100:.2f}%, Avg Recall: {avg_recall3 * 100:.2f}%, Avg Accuracy: {avg_accuracy3 * 100:.2f}%, Avg F1 Score: {avg_f13 * 100:.2f}%\")\n",
    "print(f\"ResNet18 - Avg Precision: {avg_precision4 * 100:.2f}%, Avg Recall: {avg_recall4 * 100:.2f}%, Avg Accuracy: {avg_accuracy4 * 100:.2f}%, Avg F1 Score: {avg_f14 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC curve and AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Plot ROC curves for multiple models and fill AUC areas\n",
    "def plot_roc_curves(models_roc_data):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    colors = {\n",
    "        'AlexNet': 'green',\n",
    "        'EfficientNet': 'orange',\n",
    "        'Xception': 'red',\n",
    "        'ResNet18': 'blue'\n",
    "    }\n",
    "    \n",
    "    # Plot each model's ROC curve and fill the AUC area\n",
    "    for model_name, (fpr, tpr, auc_score) in models_roc_data.items():\n",
    "        color = colors.get(model_name, 'black')  # Use black as default if the model name is not found\n",
    "        plt.plot(fpr, tpr, lw=2, color=color, label=f'{model_name} (AUC = {auc_score:.2f})')\n",
    "        # Fill the AUC area under the curve\n",
    "        #plt.fill_between(fpr, tpr, color=color, alpha=0.1)  # Add alpha for transparency\n",
    "\n",
    "    # Plot diagonal line (random guessing)\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves with AUC')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Collect ROC data for all models\n",
    "models_roc_data = {}\n",
    "\n",
    "\n",
    "\n",
    "# For Xception\n",
    "y_true, y_pred_proba = get_predictions_with_proba(xception, test_loader, device)\n",
    "fpr, tpr, auc_score = calculate_roc_auc(y_true, y_pred_proba)\n",
    "models_roc_data[\"Xception\"] = (fpr, tpr, auc_score)\n",
    "\n",
    "# For AlexNet\n",
    "y_true, y_pred_proba = get_predictions_with_proba(alexnet, test_loader, device)\n",
    "fpr, tpr, auc_score = calculate_roc_auc(y_true, y_pred_proba)\n",
    "models_roc_data[\"AlexNet\"] = (fpr, tpr, auc_score)\n",
    "\n",
    "# For ResNet18\n",
    "y_true, y_pred_proba = get_predictions_with_proba(resnet18, test_loader, device)\n",
    "fpr, tpr, auc_score = calculate_roc_auc(y_true, y_pred_proba)\n",
    "models_roc_data[\"ResNet18\"] = (fpr, tpr, auc_score)\n",
    "\n",
    "# For EfficientNet\n",
    "y_true, y_pred_proba = get_predictions_with_proba(efficientnet, test_loader, device)\n",
    "fpr, tpr, auc_score = calculate_roc_auc(y_true, y_pred_proba)\n",
    "models_roc_data[\"EfficientNet\"] = (fpr, tpr, auc_score)\n",
    "\n",
    "# Plot all ROC curves on one graph\n",
    "plot_roc_curves(models_roc_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Make Predictions with Probabilities\n",
    "def get_predictions_with_proba(model, test_loader, device):\n",
    "    y_true = []\n",
    "    y_pred_proba = []  # Store predicted probabilities\n",
    "    \n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for batch in test_loader:\n",
    "            images, labels = batch\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Forward pass: get probabilities\n",
    "            outputs = model(images)\n",
    "            probabilities = torch.sigmoid(outputs).cpu().numpy()  # Assuming binary classification\n",
    "\n",
    "            # Collect true labels and predicted probabilities\n",
    "            y_true.extend(labels.cpu().numpy().flatten())  # Flatten if necessary\n",
    "            y_pred_proba.extend(probabilities.flatten())  # Flatten if necessary\n",
    "\n",
    "    return np.array(y_true), np.array(y_pred_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Calculate ROC curve and AUC\n",
    "def calculate_roc_auc(y_true, y_pred_proba):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)  # False positive rate, true positive rate\n",
    "    auc_score = roc_auc_score(y_true, y_pred_proba)  # AUC\n",
    "    \n",
    "    return fpr, tpr, auc_score\n",
    "\n",
    "# Step 3: Plot ROC curve\n",
    "def plot_roc_curve(fpr, tpr, model_name, auc_score):\n",
    "    colors = {\n",
    "        'AlexNet': 'green',\n",
    "        'EfficientNet': 'orange',\n",
    "        'Xception': 'red',\n",
    "        'ResNet18': 'blue'\n",
    "    }\n",
    "    color = colors.get(model_name, 'black')  # Use black as default if the model name is not found\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, color=color, lw=2, label=f'ROC curve (AUC = {auc_score:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')  # Diagonal line for random guess\n",
    "    plt.fill_between(fpr, tpr, color=color, alpha=0.1)  # Add alpha for transparency\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic (ROC) - {model_name}')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred_proba = get_predictions_with_proba(alexnet, test_loader, device)\n",
    "fpr, tpr, auc_score = calculate_roc_auc(y_true, y_pred_proba)\n",
    "plot_roc_curve(fpr, tpr, \"AlexNet\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred_proba = get_predictions_with_proba(efficientnet, test_loader, device)\n",
    "fpr, tpr, auc_score = calculate_roc_auc(y_true, y_pred_proba)\n",
    "plot_roc_curve(fpr, tpr, \"EfficientNet\", auc_score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_true, y_pred_proba = get_predictions_with_proba(xception, test_loader, device)\n",
    "fpr, tpr, auc_score = calculate_roc_auc(y_true, y_pred_proba)\n",
    "plot_roc_curve(fpr, tpr, \"Xception\", auc_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred_proba = get_predictions_with_proba(resnet18, test_loader, device)\n",
    "fpr, tpr, auc_score = calculate_roc_auc(y_true, y_pred_proba)\n",
    "plot_roc_curve(fpr, tpr, \"ResNet18\", auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Plot ROC curves for multiple models and fill AUC areas\n",
    "def plot_roc_curves(models_roc_data):\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    colors = {\n",
    "        'AlexNet': 'green',\n",
    "        'EfficientNet': 'orange',\n",
    "        'Xception': 'red',\n",
    "        'ResNet18': 'blue'\n",
    "    }\n",
    "    \n",
    "    # Plot each model's ROC curve and fill the AUC area\n",
    "    for model_name, (fpr, tpr, auc_score) in models_roc_data.items():\n",
    "        color = colors.get(model_name, 'black')  # Use black as default if the model name is not found\n",
    "        plt.plot(fpr, tpr, lw=2, color=color, label=f'{model_name} (AUC = {auc_score:.2f})')\n",
    "        # Fill the AUC area under the curve\n",
    "        plt.fill_between(fpr, tpr, color=color, alpha=0.1)  # Add alpha for transparency\n",
    "\n",
    "    # Plot diagonal line (random guessing)\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curves with AUC')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "# Collect ROC data for all models\n",
    "models_roc_data = {}\n",
    "\n",
    "# For AlexNet\n",
    "y_true, y_pred_proba = get_predictions_with_proba(alexnet, test_loader, device)\n",
    "fpr, tpr, auc_score = calculate_roc_auc(y_true, y_pred_proba)\n",
    "models_roc_data[\"AlexNet\"] = (fpr, tpr, auc_score)\n",
    "\n",
    "# For EfficientNet\n",
    "y_true, y_pred_proba = get_predictions_with_proba(efficientnet, test_loader, device)\n",
    "fpr, tpr, auc_score = calculate_roc_auc(y_true, y_pred_proba)\n",
    "models_roc_data[\"EfficientNet\"] = (fpr, tpr, auc_score)\n",
    "\n",
    "# For Xception\n",
    "y_true, y_pred_proba = get_predictions_with_proba(xception, test_loader, device)\n",
    "fpr, tpr, auc_score = calculate_roc_auc(y_true, y_pred_proba)\n",
    "models_roc_data[\"Xception\"] = (fpr, tpr, auc_score)\n",
    "\n",
    "# For ResNet18\n",
    "y_true, y_pred_proba = get_predictions_with_proba(resnet18, test_loader, device)\n",
    "fpr, tpr, auc_score = calculate_roc_auc(y_true, y_pred_proba)\n",
    "models_roc_data[\"ResNet18\"] = (fpr, tpr, auc_score)\n",
    "\n",
    "# Plot all ROC curves on one graph\n",
    "plot_roc_curves(models_roc_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-H9Gen7U7DcF"
   },
   "source": [
    "### 4.2 Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wm0k5eUf7F5g"
   },
   "source": [
    "Import libraries to test and classify the images in the output folder that we downloaded using Google Street View."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713378743216,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "Vf85LW_8ocYQ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import shutil\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oBsppIW7REf"
   },
   "source": [
    "Define all the paths to the folder where we have all the images we downloaded and we want to classify and also the folders to save each image once is classified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1713378743216,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "PGJihtnp6dqA"
   },
   "outputs": [],
   "source": [
    "# Redifine you images paths\n",
    "images_dir = \"output\"\n",
    "alive_dir = \"output/alive\"\n",
    "dead_dir = \"output/dead\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redifine you images paths\n",
    "images_full_path = os.path.join(your_directory, images_dir) \n",
    "alive_full_path = os.path.join(your_directory, alive_dir)\n",
    "dead_full_path = os.path.join(your_directory, dead_dir) \n",
    "\n",
    "# Create \n",
    "if not os.path.exists(alive_full_path):\n",
    "    os.makedirs(alive_full_path)\n",
    "if not os.path.exists(dead_full_path):\n",
    "    os.makedirs(dead_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the joined path\n",
    "print(\"Images path:\", images_full_path)\n",
    "print(\"Alive Images path:\", alive_full_path)\n",
    "print(\"Dead Images path:\", dead_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 24232,
     "status": "ok",
     "timestamp": 1713378767445,
     "user": {
      "displayName": "Alba Closa Tarres",
      "userId": "12199937358227921066"
     },
     "user_tz": 240
    },
    "id": "BYwyb7N06oI5"
   },
   "outputs": [],
   "source": [
    "# List all the image files in the directory\n",
    "image_files = os.listdir(images_full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select model according to evaluation results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = efficientnet\n",
    "model_name = 'EfficientNet-B0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwUrSsuFXO1_"
   },
   "outputs": [],
   "source": [
    "# Saving the PyTorch model\n",
    "torch.save(model.state_dict(), f'{model_name}.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# converted to pytorch until here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrU3xKjK6ze3"
   },
   "outputs": [],
   "source": [
    "# List all the image files in the directory and filter out non-image files\n",
    "image_files = [f for f in os.listdir(images_full_path) if f.endswith(('.jpg', '.jpeg', '.png', '.bmp', '.tiff'))]\n",
    "\n",
    "# Initialize the dictionary to store yhat values\n",
    "yhat_values = {}\n",
    "\n",
    "alive_count = 0\n",
    "dead_count = 0\n",
    "\n",
    "# Define image transformations for PyTorch (resize, normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((512, 512)),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "for image_file in image_files:\n",
    "    # Read the image\n",
    "    img = cv2.imread(os.path.join(images_full_path, image_file))\n",
    "\n",
    "    # Check if the image is successfully loaded\n",
    "    if img is None:\n",
    "        print(f\"Error: Unable to read image {image_file}\")\n",
    "        continue  # Skip to the next image if loading fails\n",
    "\n",
    "    # Convert color space from BGR to RGB\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Apply transformations (resize, normalization, etc.)\n",
    "    img_tensor = transform(img_rgb)\n",
    "\n",
    "    # Add batch dimension\n",
    "    img_tensor = img_tensor.unsqueeze(0)\n",
    "\n",
    "    # Move tensor to device (if using GPU)\n",
    "    img_tensor = img_tensor.to(device)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        yhat = model(img_tensor)\n",
    "        yhat = F.sigmoid(yhat)  # Apply sigmoid for binary classification\n",
    "        print(yhat)\n",
    "\n",
    "    # Determine the predicted class\n",
    "    if yhat.item() > 0.695:\n",
    "        predicted_class = 'Dead'\n",
    "        destination_folder = dead_full_path\n",
    "        dead_count += 1\n",
    "    else:\n",
    "        predicted_class = 'Alive'\n",
    "        destination_folder = alive_full_path\n",
    "        alive_count += 1\n",
    "\n",
    "    # Store the yhat value in the dictionary\n",
    "    yhat_values[image_file] = yhat.item()\n",
    "\n",
    "    # Save the image to the respective classification folder\n",
    "    image_path = os.path.join(destination_folder, image_file)\n",
    "    cv2.imwrite(image_path, img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Alive Images = {alive_count}')\n",
    "print(f'Dead Images = {dead_count}')\n",
    "print(f' Images = {dead_count + alive_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "rVwl4tOKGZx1"
   },
   "outputs": [],
   "source": [
    "sorted_yhat_values_alive = dict(sorted(yhat_values.items(), key=lambda item: item[1]))\n",
    "print(sorted_yhat_values_alive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bpVHPpfEy6s4"
   },
   "source": [
    "# 5. Top 20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "Way5WLaCJs3M"
   },
   "outputs": [],
   "source": [
    "# Define the destination folder to save the images\n",
    "#destination_folder = \"C:/Users/Rojano/Desktop/alive/top20\"  # Update with the path to your destination folder\n",
    "\n",
    "# Create the destination folder if it doesn't exist\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# Counter for saving the first 20% of the images\n",
    "count = 0\n",
    "top20 =  int(len(sorted_yhat_values_alive) * 0.20)\n",
    "print(top20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through the first 20% of the images in the sorted dictionary\n",
    "for image_file, yhat_value in sorted_yhat_values_alive.items():\n",
    "    if count >= top20:\n",
    "        break\n",
    "\n",
    "    # Copy the image file to the destination folder\n",
    "    source_path = os.path.join(images_full_path, image_file)\n",
    "    destination_path = os.path.join(destination_folder, image_file)\n",
    "    shutil.copyfile(source_path, destination_path)\n",
    "\n",
    "    # Increment the counter\n",
    "    count += 1\n",
    "\n",
    "print(f\"First {count} images saved to the {destination_folder}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files and directories in the folder\n",
    "files = os.listdir(destination_folder)\n",
    "\n",
    "# Count the number of files (excluding directories)\n",
    "num_files = len(files)\n",
    "print(\"Number of files in the folder:\", num_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "output_embedded_package_id": "14fMH1R-_JMSc30sHzcCoD2Svp2dfapFR"
    },
    "id": "futATX60IS-S",
    "outputId": "f93977f7-396a-4f87-9b7a-1feac65b8603"
   },
   "outputs": [],
   "source": [
    "# Display images\n",
    "for count, image_file in enumerate(os.listdir(destination_folder), 1):\n",
    "    # Read the image\n",
    "    img = cv2.imread(os.path.join(destination_folder, image_file))\n",
    "\n",
    "    # Display the image\n",
    "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    plt.title(f\"{count}. {image_file}\")\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOALQv8LbXYvUhmf12IoTID",
   "gpuType": "T4",
   "name": "",
   "version": ""
  },
  "kernelspec": {
   "display_name": "Python (kudzu)",
   "language": "python",
   "name": "pythonproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
